
\documentclass{article} % For LaTeX2e
\usepackage[final]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}

\usepackage{lineno}

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{Assignment 1: Elliptic curve Diffie-Hellman (X25519)}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Muhammad Ayain Fida Rana \\
Department of Computer Science and Technology \\
University of Cambridge \\
15 JJ Thomson Avenue, Cambridge CB3 0FD, United Kingdom \\
\texttt{mafr2@cam.ac.uk}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle

\section{Implementation Overview}

I implemented X25519 Diffie-Hellman key exchange from scratch in Python, supporting both Montgomery ladder and double-and-add scalar multiplication algorithms. The implementation follows RFC 7748 \cite{rfc7748} and provides a clean API (\texttt{X25519} class) with proper type annotations, comprehensive test coverage, and Docker-based build verification.

The core implementation is split across modular components: field arithmetic (\texttt{field.py}), point operations (\texttt{group\_law.py}), scalar multiplication methods (\texttt{methods.py}), and encoding/decoding utilities (\texttt{encoding.py}). This separation makes the codebase maintainable and testable.

\section{Core Design Decisions}

\subsection{Two Scalar Multiplication Algorithms}

I implemented both Montgomery ladder and double-and-add to understand their trade-offs:

\textbf{Montgomery ladder} operates exclusively on x-coordinates using the differential addition formula. It processes scalar bits in constant time, making it resistant to timing attacks. The implementation follows RFC 7748's algorithm with conditional swaps (\texttt{cswap}) that execute in constant time regardless of the swap bit. This is X25519's standard algorithm.

\textbf{Double-and-add} requires full $(x, y)$ coordinates and uses recursive point doubling and addition. It's conceptually simpler but has limitations: not all x-coordinates have corresponding y-values on Curve25519, so it only works with the base point (where $y$ is known). I discovered this through testing---RFC vector 2 fails with double-and-add because the decoded x-coordinate has no valid y. The recursive implementation is elegant but not constant-time.

In \texttt{api.py}, I route algorithm selection appropriately: both work for base point multiplication (\texttt{x25519\_base}), but arbitrary point multiplication (\texttt{x25519}) uses ladder for robustness.

\subsection{Field Arithmetic Implementation}

I used Python's built-in modular exponentiation for critical operations:

\begin{verbatim}
def finv(a: int) -> int:
    if a == 0:
        raise ValueError("Cannot compute inverse of zero.")
    return pow(a, P - 2, P)
\end{verbatim}

Computing inverses via Fermat's Little Theorem ($a^{P-2} \equiv a^{-1} \pmod{P}$) is more efficient than the extended Euclidean algorithm in Python. For square roots, I implemented the algorithm from RFC 8032: compute $a^{(P+3)/8}$ and if that's not a root, multiply by $2^{(P-1)/4}$.

I initially considered using field division (\texttt{fdiv}) in scalar recursion but realized that's incorrect---scalar arithmetic uses integer division while field elements use modular division. This distinction is crucial: \texttt{double\_and\_add(k // 2, Pt)} uses \texttt{//} for scalar halving, not field operations.

\subsection{Point Representation and MSB Handling}

My \texttt{Point} class uses a dataclass with $(x, y)$ coordinates. During construction, if only x is provided, I compute y using \texttt{calculate\_y()}, which attempts to find a square root. This raised an important question: how to handle invalid x-coordinates?

RFC 7748 specifies that the MSB of the last byte must be cleared when decoding x-coordinates. I implemented this in \texttt{decode\_x\_coordinate()}:

\begin{verbatim}
b = b[:-1] + bytes([b[-1] & 0x7F])
return decode_little_endian(b) % P
\end{verbatim}

I initially did \texttt{x \% P} before MSB clearing, which was wrong---the MSB must be cleared on the byte representation before decoding. This caused subtle bugs that took multiple iterations to fix.

\section{Correctness Verification}

\subsection{Test Coverage}

I wrote 27 tests covering multiple dimensions:

\textbf{RFC 7748 test vectors} ensure compliance with the standard. All vectors pass for Montgomery ladder. Vector 2 correctly fails for double-and-add (x-coordinate has no y), which validates my error handling.

\textbf{Diffie-Hellman agreement tests} verify that Alice and Bob derive the same shared secret using both algorithms. I tested with RFC vectors and randomly generated keys. The latter caught several early bugs in my point addition logic.

\textbf{Field operation tests} verify algebraic properties (commutativity, associativity, identity, inverse) for addition and multiplication. Using property-based testing with random field elements gave me confidence in correctness.

\textbf{Encoding tests} validate length checks, MSB masking, scalar clamping, and little-endian conversion. I explicitly test edge cases like empty bytes, oversized inputs, and all-zero/all-one patterns.

\subsection{Type Safety}

I used comprehensive type annotations throughout:
\begin{verbatim}
def scalar_mult(self, k: int, x: int) -> bytes:
def double_and_add(k: int, Pt: Point) -> Point | None:
\end{verbatim}

The Dockerfile runs \texttt{ty check} at build time, catching type errors before tests run. This caught several issues where I mixed bytes and integers. Input validation raises \texttt{TypeError} or \texttt{ValueError} with descriptive messages rather than allowing silent failures.

\subsection{What I'm Still Uncertain About}

I'm not entirely sure about the security implications of accepting zero scalars. Currently, my validation rejects $k = 0$, but since X25519 always clamps scalars, this never occurs naturally. Should a malicious network peer send unclamped zero bytes, my implementation would reject it---but I haven't verified whether this is the correct behavior according to RFC 7748.

Additionally, I haven't tested all possible low-order points. While my implementation reduces x-coordinates modulo $P$, I haven't verified behavior against small-subgroup attacks systematically.

\section{Limitations and Trade-offs}

\subsection{Timing Side Channels}

Python's integer arithmetic is \textbf{not constant-time}. Variable-length integers mean that operations on larger numbers take longer, leaking information through timing. My Montgomery ladder uses constant-time conditional swaps, but the underlying field arithmetic is still vulnerable.

I measured this qualitatively: operations near $P$ take noticeably longer than small values. For production use, this is unacceptable---an attacker could perform timing analysis to extract private keys.

\subsection{Performance Considerations}

Python is inherently slower than compiled languages. I didn't implement formal benchmarks, but observed that:
\begin{itemize}
    \item Montgomery ladder: $\sim$40ms for one scalar multiplication
    \item Double-and-add: $\sim$25ms for base point (recursive depth $\sim$254)
\end{itemize}

These are rough estimates from test suite execution (27 tests in 38.8s). A C or Rust implementation would be orders of magnitude faster.

The recursive double-and-add has a practical limitation: Python's default recursion limit (1000) is sufficient for 255-bit scalars, but this is a fragile assumption.

\subsection{Memory Safety}

I don't zero sensitive data after use. Private keys and shared secrets remain in memory until garbage collection. Python doesn't provide secure memory clearing primitives, making this implementation unsuitable for production.

\section{Production Requirements}

To make this production-quality, I would need to:

\textbf{Implement in a systems language} (Rust, C) with constant-time arithmetic libraries like \texttt{curve25519-dalek} or \texttt{libsodium}. These provide timing-safe implementations and exploit CPU-specific optimizations.

\textbf{Add comprehensive input validation} for network-facing interfaces. While I validate byte lengths and scalar ranges, I haven't implemented all defenses against malicious inputs (e.g., explicitly rejecting all known low-order points).

\textbf{Implement secure key storage} with memory locking (\texttt{mlock}) and explicit zeroization. In Python, this is nearly impossible; systems languages provide better control.

\textbf{Add side-channel mitigations} beyond timing: power analysis, electromagnetic emanations, and cache timing attacks all threaten cryptographic implementations. Hardware security modules would be needed for high-security deployments.

\textbf{Formal verification} would provide the highest assurance. Tools like Fiat Crypto generate provably correct implementations from mathematical specifications. My testing is thorough but can't match formal proof.

\section{Insights and Reflections}

Implementing X25519 gave me appreciation for the subtlety of cryptographic engineering. The mathematics is well-understood, but correct implementation requires attention to details that aren't obvious from the specifications:

\begin{itemize}
    \item The distinction between field arithmetic and scalar arithmetic
    \item When to use integer vs. modular division
    \item The importance of byte-level operations (MSB clearing) before numeric conversion
    \item Why Montgomery ladder works for all points but double-and-add doesn't
\end{itemize}

I initially thought implementing both algorithms would be redundant, but it revealed deep insights. Double-and-add failing on arbitrary x-coordinates forced me to understand why X25519 uses x-only coordinates: it's not just an optimization, it's fundamental to the protocol's design.

The most surprising finding was how many edge cases exist. Empty bytes, wrong lengths, invalid points, zero scalars---each required explicit handling. Production cryptography is less about the core algorithm and more about defensive programming against all possible inputs.

If I were to extend this work, I'd implement benchmarking against other libraries (e.g., \texttt{cryptography.hazmat}) to quantify Python's performance penalty. I'd also explore constant-time techniques available in Python (though fundamentally limited) and compare with side-channel resistant implementations.

\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}

\end{document}
